{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data files and word dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lh_neg = open('../input/negative-words.txt', 'r').read()\n",
    "lh_neg = lh_neg.split('\\n')\n",
    "lh_pos = open('../input/positive-words.txt', 'r').read()\n",
    "lh_pos = lh_pos.split('\\n')\n",
    "restaurant_reviews = json.load(open(\"../input/rest_review_dictionary.json\"))\n",
    "user_reviews_json = json.load(open(\"../input/user_review_dictionary.json\"))\n",
    "word_list = list(set(lh_pos + lh_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a subset of users that have at least 200 reviews, run an iterative test on these users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 99391.09it/s]\n"
     ]
    }
   ],
   "source": [
    "bigusers = []\n",
    "for i in tqdm.tqdm(range(0, len(user_reviews_json.keys()))):\n",
    "    if len(user_reviews_json[user_reviews_json.keys()[i]]) > 100:\n",
    "        bigusers.append(user_reviews_json.keys()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a specific user and start building out the recommendation on her data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_specific_reviews = user_reviews_json[bigusers[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a separate list for the review text and the review ratings, then aggregate them into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_reviews = []\n",
    "user_ratings = []\n",
    "business_ids = []\n",
    "\n",
    "for review in user_specific_reviews:\n",
    "    user_reviews.append(review['text'])\n",
    "    user_ratings.append(review['stars'])\n",
    "    business_ids.append(review['business_id'])\n",
    "\n",
    "user_reviews = [review.encode('utf-8').translate(None, string.punctuation) for review in user_reviews]\n",
    "    \n",
    "user_df = pd.DataFrame({'review_text': user_reviews, 'rating': user_ratings, 'biz_id': business_ids})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the mongodb and pull in the relevant restaurant reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Start a connection with the AWS instance and pull in the business reviews database\n",
    "ip = '54.146.170.140'\n",
    "conn = MongoClient(ip, 27017)\n",
    "conn.database_names()\n",
    "db = conn.get_database('cleaned_data')\n",
    "reviews = db.get_collection('restaurant_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [03:54<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "restreview = {}\n",
    "for i in tqdm.tqdm(range(0, len(business_ids))):\n",
    "    rlist = []\n",
    "    for obj in reviews.find({'business_id':business_ids[i]}):\n",
    "        rlist.append(obj)\n",
    "    restreview[business_ids[i]] = rlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a separate list for each review for the businesses that show up in the business_id list. Remove all reviews that relate to the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [00:00<00:00, 2542.76it/s]\n"
     ]
    }
   ],
   "source": [
    "user_id = user_reviews_json.keys()[29]\n",
    "rest_reviews = []\n",
    "rest_ratings = []\n",
    "biz_ids = []\n",
    "for i in tqdm.tqdm(range(0, len(restreview.keys()))):\n",
    "    for restaurant in restreview[restreview.keys()[i]]:\n",
    "        if restaurant['user_id'] != user_id:\n",
    "            rest_reviews.append(restaurant['text'])\n",
    "            rest_ratings.append(restaurant['stars'])\n",
    "            biz_ids.append(restreview.keys()[i])\n",
    "        else:\n",
    "            pass\n",
    "restaurant_df = pd.DataFrame({'review_text': rest_reviews, 'rating': rest_ratings, 'biz_id': biz_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in review.lower().split() if (word not in stop_words)]\n",
    "  for review in train_reviews]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "class LSATransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, reviews):\n",
    "        texts = [[word for word in review.lower().split() if (word not in stop_words)]\n",
    "          for review in reviews]\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        reviews_tfidf = tfidf[corpus]\n",
    "        index = similarities.MatrixSimilarity(lsi[reviews_tfidf])\n",
    "        lsi_vector = index[lsi[reviews_tfidf]]\n",
    "        return lsa_vector\n",
    "\n",
    "    def fit(self, reviews, y=None):\n",
    "        texts = [[word for word in review.lower().split() if (word not in stop_words)]\n",
    "          for review in reviews]\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        corpus_tfidf = tfidf[corpus]\n",
    "        lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=30)\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feature objects and functions\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def sent_percent(review):\n",
    "    regex_words = re.compile('[a-z]+')\n",
    "    words = [x.lower() for x in review.split(' ')]\n",
    "    words = [x for x in words if regex_words.match(x)]\n",
    "    pos_count, neg_count = 0, 0\n",
    "    for word in words:\n",
    "        if word in lh_pos:\n",
    "            pos_count += 1\n",
    "        elif word in lh_neg:\n",
    "            neg_count += 1\n",
    "    return [float(pos_count)/float(len(words)), float(neg_count)/float(len(words))]\n",
    "\n",
    "pos_vectorizer = CountVectorizer(vocabulary = lh_pos)\n",
    "neg_vectorizer = CountVectorizer(vocabulary = lh_neg)\n",
    "class SentimentPercentage(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, reviews):\n",
    "        ##Take in a list of textual reviews and return a list with two elements:\n",
    "        ##[Positive Percentage, Negative Percentage]\n",
    "        pos_vect = pos_vectorizer.transform(reviews)\n",
    "        neg_vect = neg_vectorizer.transform(reviews)\n",
    "        features = []\n",
    "        \n",
    "        for i in range(0, len(reviews)):\n",
    "            sent_percentage = []\n",
    "            sent_percentage.append(float(pos_vect[i].sum())/float(len(reviews[i])))\n",
    "            sent_percentage.append(float(neg_vect[i].sum())/float(len(reviews[i])))\n",
    "            features.append(sent_percentage)\n",
    "            \n",
    "        return np.array(features)\n",
    "\n",
    "    def fit(self, reviews, y=None, n_grams = None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class TfIdfGramTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, reviews):\n",
    "        tf_vector = vectorizer.transform(reviews)\n",
    "        return tf_vector\n",
    "\n",
    "    def fit(self, reviews, y=None, n_grams = (1,1)):\n",
    "        vectorizer = TfidfVectorizer(ngram_range = n_grams, stop_words = 'english')\n",
    "        vectorizer.fit(reviews)\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return vectorizer\n",
    "    \n",
    "class LSATransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, reviews):\n",
    "        tf_vector = vectorizer.transform(reviews)\n",
    "        return tf_vector\n",
    "\n",
    "    def fit(self, reviews, y=None, n_grams = (1,1)):\n",
    "        vectorizer = TfidfVectorizer(ngram_range = n_grams, stop_words = 'english')\n",
    "        vectorizer.fit(reviews)\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Whether or not a user likes a recommendation is hard to capture because we don't know if they like it or not until after we recommend it. In the above design, we'd be recommending a _new_ restaurant to the user. To see if they actually like it, we'd have to follow up after we make the recommendation and ask them how they felt. But this isn't feasible because we don't have a set of people we can just ask how they felt.\n",
    "\n",
    "That is, we know $y_{pred}$ but we don't know $y_{actual}$. \n",
    "\n",
    "But we can get around this because sometimes a user review rating is also that user's restaurant rating. For a given user, if she only has one review for a restaurant then the rating for that review is also her rating for the restaurant.\n",
    "\n",
    "We can use this to test our recommendation system. We propose the following test design:\n",
    "\n",
    "**Build:**\n",
    "* As before, let $B$ be the total set of user reviews.\n",
    "* Let $R$ be the set of restaurants that the user has reviewed only once.\n",
    "* Take some percentage, $p$, of the set $R$ and take the subset of reviews from $B$ that correspond to these restaurants. Let this be the set of test restaurants $B_{test}$.\n",
    "* Set the remaining $1-p$ percentage of the set $R$ and call this the training set of restaurants $R_{train}$. \n",
    "* Note every restaurant in $R_{train}$ and $B_{test}$ has User's Restaurant Rating = User's Review Rating\n",
    "\n",
    "**Run:**\n",
    "1. For each review in $B_{test}$, create the tuple (User Review, Restaurant Rating, Restaurant ID) and replace the instance in $B_{test}$ with the tuple.\n",
    "\n",
    "2. For each restaurant in $R_{train}$, find the total set of reviews from the Reviews database. Let this set be $Y$, where each element in $Y$ is a tuple (User Review, Restaurant Rating, Restaurant ID)\n",
    "\n",
    "3. We run each of the algorithms above, using $Y$ as the training set and $B_{test}$ as the test set.\n",
    "4. Step 3 results in a set $B_{result}$ where each element is characterized by (User Review, Actual Restaurant Rating, Predicted Restaurant Rating, Predicted Restaurant). Note the cardinality of $B_{result}$ is the same as that of $B_{test}$\n",
    "5. Set $y_{pred} =$ I(Predicted Restaurant) and $y_{actual} =$ I(Restaurant) where the indicator function I() is 1 if the user rated the restaurant at least a 4 and is 0 if the user rated the restaurant less than a 4\n",
    "6. The RMSE for the recommended restaurants is given by the following loss function:\n",
    "\n",
    "$$RMSE = \\sum_{i=1}^{N} \\sqrt{\\frac{(y_{i, pred} - y_{i, actual})^{2}}{N}}$$\n",
    "\n",
    "Where N is the number of recommended restaurants in $B_{result}$. $y_{i, pred}$ is the predicted restaurant rating, $y_{i, actual}$ is the actual rating that the user gave to the restaurant. A RMSE score of 0 is a perfect score and means that the recommendation system did really well. In this case, success means that the recommendation system was able to accurately predict how the user would feel about the restaurant on a binary scale (good or bad).\n",
    "\n",
    "Note, this is equivalent to using the mean absolute error because of our label construction. This function is analagous to the mean squared error loss function used in the 2013 Yelp RecSys challenge with the difference being that $y_{i, pred}$ and $y_{i, actual}$ are discrete categorical variables $\\in \\{1, 2, 3, 4, 5\\}$:\n",
    "\n",
    "$$RMSE =  \\sum_{i=1}^{N} \\sqrt{\\frac{(y_{i, pred} - y_{i, actual})^{2}}{N}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start by splitting the restaurants that the user has reviewed into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_set = business_ids[632:633]\n",
    "# training_set = business_ids[0:632]\n",
    "\n",
    "# test_reviews, test_ratings, train_reviews, train_ratings = [], [], [], []\n",
    "\n",
    "# for rest_id in test_set:\n",
    "#     test_reviews.extend(list(restaurant_df[restaurant_df['biz_id'] == rest_id]['review_text']))\n",
    "#     test_ratings.extend(list(restaurant_df[restaurant_df['biz_id'] == rest_id]['rating']))\n",
    "    \n",
    "# for rest_id in training_set:\n",
    "#     train_reviews.extend(list(user_df[user_df['biz_id'] == rest_id]['review_text']))\n",
    "#     train_ratings.extend(list(user_df[user_df['biz_id'] == rest_id]['rating']))\n",
    "\n",
    "# #Transform the star labels into a binary class problem, 0 if rating is < 4 else 1\n",
    "# train_labels = [1 for x in train_ratings if x >=4] + [0 for x in train_ratings if x < 4] \n",
    "# test_labels = [1 for x in test_ratings if x >=4] + [0 for x in test_ratings if x < 4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Now, let's start with our tf-idf features. We train only on the user reviews within the training set \n",
    "# #and then test using the restaurant reviews. We take a majority vote on the restaurant reviews, and return that as \n",
    "# #the predicted user rating.\n",
    "\n",
    "# vectorizer = TfidfVectorizer(ngram_range = (1,1), stop_words = 'english')\n",
    "# vectorizer.fit(test_reviews)\n",
    "\n",
    "# comb_features = FeatureUnion([('tf', TfIdfGramTransformer()), \n",
    "#                               ('sent_percent',SentimentPercentage())\n",
    "#                              ])\n",
    "\n",
    "# train_features = comb_features.transform(train_reviews)\n",
    "# test_features = comb_features.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Benchmark is simply 50/50 for each prediciton, so let's take a look at the log loss for that case\n",
    "benchmark_results = [0.5] * len(test_labels)\n",
    "print \"The number to beat is: \" + str(log_loss(test_labels, benchmark_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ##########################################\n",
    "# # Running XGBoost\n",
    "# ##########################################\n",
    "# # Set parameters for XGBoost\n",
    "# # test-logloss:0.63119 using tfidf and sentiment percentage\n",
    "# params = {}\n",
    "# params['objective'] = 'binary:logistic'\n",
    "# params['eval_metric'] = 'logloss'\n",
    "# params['eta'] = 0.02\n",
    "# params['max_depth'] = 10\n",
    "\n",
    "# d_train = xgb.DMatrix(train_features, label=train_labels)\n",
    "# d_test = xgb.DMatrix(test_features, label=test_labels)\n",
    "\n",
    "# watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "# bst = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=100, verbose_eval=10)\n",
    "# ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now, let's do this process iteratively for a larger sub sample of test reviews\n",
    "#First, start by splitting the restaurants that the user has reviewed into training and test sets\n",
    "split_samp = .20\n",
    "test_set = business_ids[0:int(len(business_ids) * split_samp)]\n",
    "training_set = business_ids[int(len(business_ids) * split_samp): len(business_ids)]\n",
    "train_reviews, train_ratings = [], []\n",
    "\n",
    "for rest_id in training_set:\n",
    "    train_reviews.extend(list(user_df[user_df['biz_id'] == rest_id]['review_text']))\n",
    "    train_ratings.extend(list(user_df[user_df['biz_id'] == rest_id]['rating']))\n",
    "\n",
    "#Transform the star labels into a binary class problem, 0 if rating is < 4 else 1\n",
    "train_labels = [1 if x >=4 else 0 for x in train_ratings ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization ideas:\n",
    "1. The results across all users. Which ML algorithms performed best? Was there a clear winner or did people differ?\n",
    "2. Show the performance of the algorithms across accuracy types (log-loss, mean squared error, accuracy score). Note that in our case accuracy score = 1 - mean squared error and that mean squared error = abs error \n",
    "    * Log-loss will be more informative in our case. The probabilities will be different depending on the algorithm, we can get a measure of \"how confident\" our algorithms are\n",
    "3. Graph the mean squared error against the threshold probability we use to classify as 0 or 1\n",
    "\n",
    "Choosing the top 50 by probability performs better than focusing on the entire set, show this graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "####LSI Features\n",
    "###########################\n",
    "texts = [[word for word in review.lower().split() if (word not in stop_words)]\n",
    "          for review in train_reviews]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "numpy_matrix = matutils.corpus2dense(corpus, num_terms=10000)\n",
    "singular_values = np.linalg.svd(numpy_matrix, full_matrices=False, compute_uv=False)\n",
    "mean_sv = sum(list(singular_values))/len(singular_values)\n",
    "topics = int(mean_sv)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=topics)\n",
    "index = similarities.MatrixSimilarity(lsi[corpus_tfidf]) \n",
    "\n",
    "train_lsi = lsi[corpus_tfidf]\n",
    "train_lsi = [[train[1] for train in train_review] for train_review in train_lsi]\n",
    "train_lsi = [[0.0000000001] * topics if len(x) != topics else x for x in train_lsi]\n",
    "train_lsi = sparse.coo_matrix(train_lsi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:05<00:00,  7.98it/s]\n"
     ]
    }
   ],
   "source": [
    "train_features = train_lsi\n",
    "#XGBoost training\n",
    "#gbm = xgb.XGBClassifier(max_depth=10, n_estimators=400, learning_rate=0.02).fit(train_features, train_labels)\n",
    "#RandomForest training\n",
    "#rf = RandomForestClassifier()\n",
    "rf.fit(train_features, train_labels)\n",
    "#SVM training\n",
    "svm_classifier = svm.SVC(kernel='linear')\n",
    "svm_classifier.fit(train_features, train_labels)\n",
    "error = []\n",
    "for i in tqdm.tqdm(range(0,len(test_set))):\n",
    "    predicted_rating = 0\n",
    "    #Get reviews for that restaurant\n",
    "    test_reviews =[]\n",
    "    test_reviews.extend(list(restaurant_df[restaurant_df['biz_id'] == test_set[i]]['review_text']))\n",
    "    \n",
    "    #Transform features\n",
    "    test_features = comb_features.transform(test_reviews)\n",
    "    \n",
    "    #LSI Features\n",
    "    test_texts = [[word for word in test_set[i].lower().split() if (word not in stop_words)]\n",
    "          for review in test_reviews]\n",
    "    test_corpus = [dictionary.doc2bow(test) for test in test_texts]\n",
    "    test_tfidf = tfidf[test_corpus]\n",
    "    test_lsi = lsi[test_tfidf]\n",
    "    test_lsi = [[test[1] for test in test_review] for test_review in test_lsi]\n",
    "    test_lsi = [[0.0000000001] * topics if len(x) != topics else x for x in test_lsi]\n",
    "    \n",
    "    test_lsi = sparse.coo_matrix(test_lsi)\n",
    "    #stacked_test_features = sparse.hstack((test_features, test_lsi))\n",
    "    stacked_test_features = test_lsi\n",
    "    #Get XGBoost prediction\n",
    "    #test_prediction = gbm.predict(stacked_test_features)\n",
    "    #Get SVM prediction\n",
    "    #test_prediction = svm_classifier.predict(stacked_test_features)\n",
    "    #Get Random Forest prediction\n",
    "    test_prediction = rf.predict(stacked_test_features)   \n",
    "\n",
    "    if test_prediction.mean() > 0.7:\n",
    "        predicted_rating = 1\n",
    "\n",
    "    actual_rating = list(user_df[user_df['biz_id'] == test_set[i]]['rating'])[0]\n",
    "    if actual_rating >= 4:\n",
    "        actual_rating = 1\n",
    "    else:\n",
    "        actual_rating = 0\n",
    "\n",
    "    error.append(abs(predicted_rating - actual_rating))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LSA mean absolute error is: 0.533333333333\n"
     ]
    }
   ],
   "source": [
    "print \"The LSA mean absolute error is: \" + str(sum(error) / float(len(error)))\n",
    "#print \"The svm (1,1) average mean absolute error is: \" + str(sum(svm_error) / float(len(svm_error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsonwang/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "100%|██████████| 45/45 [00:04<00:00,  9.77it/s]\n"
     ]
    }
   ],
   "source": [
    "comb_features = FeatureUnion([('sent_percent',SentimentPercentage()),('tf', TfIdfGramTransformer()), \n",
    "                              ('lda', Pipeline([('bow', TfidfVectorizer(stop_words='english',ngram_range=(1,1))), \n",
    "                                        ('lda_transform', LatentDirichletAllocation(n_topics=int(mean_sv)))]))\n",
    "                             ])\n",
    "\n",
    "comb_features.fit(train_reviews)\n",
    "train_features = comb_features.transform(train_reviews)\n",
    "\n",
    "#XGBoost training\n",
    "#gbm = xgb.XGBClassifier(max_depth=10, n_estimators=400, learning_rate=0.02).fit(train_features, train_labels)\n",
    "#RandomForest training\n",
    "#rf = RandomForestClassifier()\n",
    "rf.fit(train_features, train_labels)\n",
    "#SVM training\n",
    "#svm_classifier = svm.SVC(kernel='linear')\n",
    "svm_classifier.fit(train_features, train_labels)\n",
    "test_error = []\n",
    "for i in tqdm.tqdm(range(0,len(test_set))):\n",
    "    predicted_rating = 0\n",
    "    #Get reviews for that restaurant\n",
    "    test_reviews =[]\n",
    "    test_reviews.extend(list(restaurant_df[restaurant_df['biz_id'] == test_set[i]]['review_text']))\n",
    "    \n",
    "    #Transform features\n",
    "    stacked_test_features = comb_features.transform(test_reviews)\n",
    "    \n",
    "    #Get XGBoost prediction\n",
    "    #test_prediction = gbm.predict(stacked_test_features)\n",
    "    #Get SVM prediction\n",
    "    #test_prediction = svm_classifier.predict(stacked_test_features)\n",
    "    #Get Random Forest prediction\n",
    "    test_prediction = rf.predict(stacked_test_features)   \n",
    "    if test_prediction.mean() > 0.7:\n",
    "        predicted_rating = 1\n",
    "\n",
    "    actual_rating = list(user_df[user_df['biz_id'] == test_set[i]]['rating'])[0]\n",
    "    if actual_rating >= 4:\n",
    "        actual_rating = 1\n",
    "    else:\n",
    "        actual_rating = 0\n",
    "\n",
    "    test_error.append(abs(predicted_rating - actual_rating))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LDA mean absolute error is: 0.511111111111\n"
     ]
    }
   ],
   "source": [
    "print \"The LDA mean absolute error is: \" + str(sum(test_error) / float(len(test_error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsonwang/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "100%|██████████| 45/45 [00:05<00:00,  8.39it/s]\n"
     ]
    }
   ],
   "source": [
    "comb_features = FeatureUnion([('sent_percent',SentimentPercentage()),('tf', TfIdfGramTransformer()), \n",
    "                              ('lda', Pipeline([('bow', TfidfVectorizer(stop_words='english', ngram_range = (1,1))), \n",
    "                                        ('lda_transform', LatentDirichletAllocation(n_topics=500))]))\n",
    "                             ])\n",
    "\n",
    "comb_features.fit(train_reviews)\n",
    "train_features = comb_features.transform(train_reviews)\n",
    "train_features = sparse.hstack((train_features, train_lsi))\n",
    "\n",
    "#XGBoost training\n",
    "#gbm = xgb.XGBClassifier(max_depth=10, n_estimators=500, learning_rate=0.02, ).fit(train_features, train_labels)\n",
    "#RandomForest training\n",
    "#rf = RandomForestClassifier()\n",
    "#rf.fit(train_features, train_labels)\n",
    "#SVM training\n",
    "svm_classifier = svm.SVC(kernel='linear')\n",
    "svm_classifier.fit(train_features, train_labels)\n",
    "comb_error = []\n",
    "test_predictions = []\n",
    "for i in tqdm.tqdm(range(0,len(test_set))):\n",
    "    predicted_rating = 0\n",
    "    #Get reviews for that restaurant\n",
    "    test_reviews =[]\n",
    "    test_reviews.extend(list(restaurant_df[restaurant_df['biz_id'] == test_set[i]]['review_text']))\n",
    "    \n",
    "    #Transform features\n",
    "    test_features = comb_features.transform(test_reviews)\n",
    "    \n",
    "    #LSI Features\n",
    "    test_texts = [[word for word in test_set[i].lower().split() if (word not in stop_words)]\n",
    "          for review in test_reviews]\n",
    "    test_corpus = [dictionary.doc2bow(test) for test in test_texts]\n",
    "    test_tfidf = tfidf[test_corpus]\n",
    "    test_lsi = lsi[test_tfidf]\n",
    "    test_lsi = [[test[1] for test in test_review] for test_review in test_lsi]\n",
    "    test_lsi = [[0.000000000001] * topics if len(x) != topics else x for x in test_lsi]\n",
    "    \n",
    "    test_lsi = sparse.coo_matrix(test_lsi)\n",
    "    stacked_test_features = sparse.hstack((test_features, test_lsi))\n",
    "\n",
    "    #Get XGBoost prediction\n",
    "    #test_prediction = gbm.predict(stacked_test_features)\n",
    "    #Get SVM prediction\n",
    "    test_prediction = svm_classifier.predict(stacked_test_features)\n",
    "    #Get Random Forest prediction\n",
    "    #test_prediction = rf.predict(stacked_test_features)   \n",
    "    \n",
    "    if test_prediction.mean() > 0.7:\n",
    "        predicted_rating = 1\n",
    "\n",
    "    actual_rating = list(user_df[user_df['biz_id'] == test_set[i]]['rating'])[0]\n",
    "    if actual_rating >= 4:\n",
    "        actual_rating = 1\n",
    "    else:\n",
    "        actual_rating = 0\n",
    "        \n",
    "    test_predictions.append((test_prediction, actual_rating))\n",
    "    \n",
    "    comb_error.append(abs(predicted_rating - actual_rating))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LDA + LSA mean absolute error is: 0.377777777778\n"
     ]
    }
   ],
   "source": [
    "print \"The LDA + LSA mean absolute error is: \" + str(sum(comb_error) / float(len(comb_error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a rec accuracy of: 0.8\n"
     ]
    }
   ],
   "source": [
    "confidence_tuple = [(float(sum(list(x[0])))/float(len(x[0])),x[1]) for x in test_predictions]\n",
    "confidence_tuple.sort()\n",
    "top5 = confidence_tuple[-10:]\n",
    "print \"Got a rec accuracy of: \" + str(float(sum([x[1] for x in confidence_tuple[-10:]]))/float(10))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
