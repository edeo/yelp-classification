{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn_deltatfidf import DeltaTfidfVectorizer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "sys.path.append('/Users/robertsonwang/Desktop/Python/Yelp_class/yelp-classification/machine_learning')\n",
    "import yelp_ml as yml\n",
    "reload(yml)\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary dictionaries and JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lh_neg = open('../input/negative-words.txt', 'r').read()\n",
    "lh_neg = lh_neg.split('\\n')\n",
    "lh_pos = open('../input/positive-words.txt', 'r').read()\n",
    "lh_pos = lh_pos.split('\\n')\n",
    "users = json.load(open(\"cleaned_large_user_dictionary.json\"))\n",
    "word_list = list(set(lh_pos + lh_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-95d6ab29c6cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0muser_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlist_reviews\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0muser_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_reviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#We have 228 users, creat a new dictionary where the user_ids are the keys and the entries are a list of reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'reviews'"
     ]
    }
   ],
   "source": [
    "#Fix users JSON\n",
    "users_dict = {}\n",
    "user_ids = []\n",
    "\n",
    "for list_reviews in users['reviews']:\n",
    "    user_ids.append(list_reviews[0]['user_id'])\n",
    "#We have 228 users, creat a new dictionary where the user_ids are the keys and the entries are a list of reviews\n",
    "for i in tqdm.tqdm(range(0, len(user_ids))):\n",
    "    users_dict[user_ids[i]] = users['reviews'][i]\n",
    "with open('cleaned_large_user_dictionary.json', 'wb') as outfile:\n",
    "    json.dump(users_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running a few tests on a subset of users, the keys are our unique user IDs. We proceed as follows for each user ID:\n",
    "1. Create a user dataframe with the following columns:\n",
    "    * (review_text, review rating, business_id)\n",
    "2. Create a list of unique business IDs for that user\n",
    "3. Connect to the MongoDB server and pull all of the reviews for the restaurants that the user has reviewed\n",
    "4. Create a restaurant dataframe with the following columns:\n",
    "    * (review_text, biz rating, business_id)\n",
    "5. Do a 80/20 training/test split, randomizing over the set of user' reviewed restaurants\n",
    "6. Train the LSI model on the set of training reviews, get the number of topics used in fitting\n",
    "7. Set up the FeatureUnion with the desired features, then fit according to the train reviews and transform the train reviews\n",
    "8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsonwang/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#####Pull restaurant data for a given user\n",
    "ip = '184.73.129.244'\n",
    "conn = MongoClient(ip, 27017)\n",
    "conn.database_names()\n",
    "db = conn.get_database('cleaned_data')\n",
    "reviews = db.get_collection('restaurant_reviews')\n",
    "\n",
    "for user in users.keys()[0:1]:\n",
    "    user_df = yml.make_user_df(users[user])\n",
    "    business_ids = list(set(user_df['biz_id']))\n",
    "#     restreview = {}\n",
    "#     for i in tqdm.tqdm(range(0, len(business_ids))):\n",
    "#         rlist = []\n",
    "#         for obj in reviews.find({'business_id':business_ids[i]}):\n",
    "#             rlist.append(obj)\n",
    "#         restreview[business_ids[i]] = rlist\n",
    "#     restaurant_df = yml.make_biz_df(user, restreview)\n",
    "    \n",
    "#     #Create a training and test sample from the user reviewed restaurants\n",
    "#     split_samp = .20\n",
    "#     random_int = random.randint(1, len(business_ids)-1)\n",
    "#     len_random = int(len(business_ids) * split_samp)\n",
    "#     test_set = business_ids[random_int:random_int+len_random]\n",
    "#     training_set = business_ids[0:random_int]+business_ids[random_int+len_random:len(business_ids)]\n",
    "#     train_reviews, train_ratings = [], []\n",
    "    \n",
    "    \n",
    "#     #Create a list of training reviews and training ratings\n",
    "#     for rest_id in training_set:\n",
    "#         train_reviews.extend(list(user_df[user_df['biz_id'] == rest_id]['review_text']))\n",
    "#         train_ratings.extend(list(user_df[user_df['biz_id'] == rest_id]['rating']))\n",
    "    \n",
    "#     #Transform the star labels into a binary class problem, 0 if rating is < 4 else 1\n",
    "#     train_labels = [1 if x >=4 else 0 for x in train_ratings]\n",
    "    \n",
    "#     #Fit LSI model and return number of LSI topics\n",
    "#     lsi, topics, dictionary = yml.fit_lsi(train_reviews)\n",
    "    \n",
    "#     #Make a FeatureUnion object with the desired features then fit to train reviews\n",
    "#     comb_features = yml.make_featureunion(lda=False)\n",
    "#     comb_features.fit(train_reviews)\n",
    "    \n",
    "#     train_features = comb_features.transform(train_reviews)\n",
    "#     train_lsi = yml.get_lsi_features(train_reviews, lsi, topics, dictionary)\n",
    "#     train_features = sparse.hstack((train_features, train_lsi))\n",
    "#     train_features = train_features.todense()\n",
    "\n",
    "    #Create a training and test sample from the user reviewed restaurants\n",
    "    split_samp = .30\n",
    "    random_int = random.randint(1, len(business_ids)-1)\n",
    "    len_random = int(len(business_ids) * split_samp)\n",
    "    test_set = business_ids[random_int:random_int+len_random]\n",
    "    training_set = business_ids[0:random_int]+business_ids[random_int+len_random:len(business_ids)]\n",
    "    sub_train_reviews, train_labels, train_reviews, train_ratings = [], [], [], []\n",
    "\n",
    "\n",
    "    #Create a list of training reviews and training ratings\n",
    "    for rest_id in training_set:\n",
    "        train_reviews.append((user_df[user_df['biz_id'] == rest_id]['review_text'].iloc[0],\n",
    "                                 user_df[user_df['biz_id'] == rest_id]['rating'].iloc[0]))\n",
    "\n",
    "    #Create an even sample s.t. len(positive_reviews) = len(negative_reviews)\n",
    "    sample_size = min(len([x[1] for x in train_reviews if x[1] < 4]),\n",
    "                          len([x[1] for x in train_reviews if x[1] >= 4]))\n",
    "    bad_reviews = [x for x in train_reviews if x[1] < 4]\n",
    "    good_reviews = [x for x in train_reviews if x[1] >= 4]\n",
    "\n",
    "    for i in range(0, int(float(sample_size)/float(2))):\n",
    "        sub_train_reviews.append(bad_reviews[i][0])\n",
    "        sub_train_reviews.append(good_reviews[i][0])\n",
    "        train_labels.append(bad_reviews[i][1])\n",
    "        train_labels.append(good_reviews[i][1])\n",
    "    \n",
    "    #Transform the star labels into a binary class problem, 0 if rating is < 4 else 1\n",
    "    train_labels = [1 if x >=4 else 0 for x in train_labels]\n",
    "\n",
    "    #Fit LSI model and return number of LSI topics\n",
    "    lsi, topics, dictionary = yml.fit_lsi(sub_train_reviews)\n",
    "    \n",
    "    #Fit DeltaTFIDF Vecotrizer\n",
    "    delta_vect = DeltaTfidfVectorizer(stop_words = 'english')\n",
    "    delta_tfidf_vect = delta_vect.fit_transform(sub_train_reviews,train_labels)\n",
    "    \n",
    "    #Make a FeatureUnion object with the desired features then fit to train reviews\n",
    "    comb_features = yml.make_featureunion()\n",
    "    comb_features.fit(sub_train_reviews)\n",
    "\n",
    "    train_features = comb_features.transform(sub_train_reviews)\n",
    "    train_lsi = yml.get_lsi_features(sub_train_reviews, lsi, topics, dictionary)\n",
    "    train_features = sparse.hstack((train_features, train_lsi, delta_tfidf_vect))\n",
    "    train_features = train_features.todense()\n",
    "    \n",
    "    #fit each model in turn \n",
    "    model_runs = [(True, False, False), \n",
    "                  (False, True, False), \n",
    "                  (False, False, True)]\n",
    "    test_results = {}\n",
    "    for i in tqdm.tqdm(range(0, len(model_runs))):\n",
    "        clf = yml.fit_model(train_features, train_labels, svm_clf = model_runs[i][0], \n",
    "                        RandomForest = model_runs[i][1], nb = model_runs[i][2])\n",
    "        threshold = 0.7\n",
    "        error = yml.test_user_set(test_set, clf, restaurant_df, user_df, comb_features, \n",
    "                                  threshold, lsi, topics, dictionary, delta_vect)\n",
    "        test_results[clf] = error\n",
    "    \n",
    "    #Get scores\n",
    "    for key in test_results.keys():\n",
    "        results = test_results[key]\n",
    "        log_loss = yml.get_log_loss(results)\n",
    "        print \"The log loss score is: \" + str(log_loss)\n",
    "        accuracy = yml.get_accuracy_score(results)\n",
    "        print \"The accuracy score is: \" + str(accuracy)\n",
    "        precision = yml.get_precision_score(results)\n",
    "        print \"The precision score is: \" + str(precision)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
