{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_deltatfidf import DeltaTfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "sys.path.append('../machine_learning')\n",
    "import yelp_ml as yml\n",
    "reload(yml)\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lh_neg = open('../input/negative-words.txt', 'r').read()\n",
    "lh_neg = lh_neg.split('\\n')\n",
    "lh_pos = open('../input/positive-words.txt', 'r').read()\n",
    "lh_pos = lh_pos.split('\\n')\n",
    "users = json.load(open(\"cleaned_large_user_dictionary.json\"))\n",
    "word_list = list(set(lh_pos + lh_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip = '184.73.129.244'\n",
    "conn = MongoClient(ip, 27017)\n",
    "db = conn.get_database('cleaned_data')\n",
    "reviews = db.get_collection('restaurant_reviews')\n",
    "users_results = {}\n",
    "user_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A/Users/robertsonwang/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "for j in tqdm.tqdm(range(0, len(users.keys()[1:2]))):\n",
    "    user_df = yml.make_user_df(users[users.keys()[j]])\n",
    "    business_ids = list(set(user_df['biz_id']))\n",
    "#     restreview = {}\n",
    "\n",
    "#     for i in range(0, len(business_ids)):\n",
    "#         rlist = []\n",
    "#         for obj in reviews.find({'business_id':business_ids[i]}):\n",
    "#             rlist.append(obj)\n",
    "#         restreview[business_ids[i]] = rlist\n",
    "#     restaurant_df = yml.make_biz_df(users.keys()[j], restreview)\n",
    "    \n",
    "    #Create a training and test sample from the user reviewed restaurants\n",
    "    split_samp = .30\n",
    "    random_int = random.randint(1, len(business_ids)-1)\n",
    "    len_random = int(len(business_ids) * split_samp)\n",
    "    test_set = business_ids[random_int:random_int+len_random]\n",
    "    training_set = business_ids[0:random_int]+business_ids[random_int+len_random:len(business_ids)]\n",
    "    train_reviews, train_ratings = [], []\n",
    "    \n",
    "    #Create a list of training reviews and training ratings\n",
    "    for rest_id in training_set:\n",
    "        train_reviews.extend(list(user_df[user_df['biz_id'] == rest_id]['review_text']))\n",
    "        train_ratings.extend(list(user_df[user_df['biz_id'] == rest_id]['rating']))\n",
    "    \n",
    "    #Transform the star labels into a binary class problem, 0 if rating is < 4 else 1\n",
    "    train_labels = [1 if x >=4 else 0 for x in train_ratings]\n",
    "    \n",
    "    #Fit LSI model and return number of LSI topics\n",
    "    lsi, topics, dictionary = yml.fit_lsi(train_reviews)\n",
    "\n",
    "    #Create a training and test sample from the user reviewed restaurants\n",
    "    split_samp = .30\n",
    "    random_int = random.randint(1, len(business_ids)-1)\n",
    "    len_random = int(len(business_ids) * split_samp)\n",
    "    test_set = business_ids[random_int:random_int+len_random]\n",
    "    training_set = business_ids[0:random_int]+business_ids[random_int+len_random:len(business_ids)]\n",
    "    sub_train_reviews, train_labels, train_reviews, train_ratings = [], [], [], []\n",
    "\n",
    "    #Create a list of training reviews and training ratings\n",
    "    for rest_id in training_set:\n",
    "        train_reviews.append((user_df[user_df['biz_id'] == rest_id]['review_text'].iloc[0],\n",
    "                                 user_df[user_df['biz_id'] == rest_id]['rating'].iloc[0]))\n",
    "\n",
    "    #Create an even sample s.t. len(positive_reviews) = len(negative_reviews)\n",
    "    sample_size = min(len([x[1] for x in train_reviews if x[1] < 4]),\n",
    "                          len([x[1] for x in train_reviews if x[1] >= 4]))\n",
    "    \n",
    "    bad_reviews = [x for x in train_reviews if x[1] < 4]\n",
    "    good_reviews = [x for x in train_reviews if x[1] >= 4]\n",
    "\n",
    "    for L in range(0, int(float(sample_size)/float(2))):\n",
    "        sub_train_reviews.append(bad_reviews[L][0])\n",
    "        sub_train_reviews.append(good_reviews[L][0])\n",
    "        train_labels.append(bad_reviews[L][1])\n",
    "        train_labels.append(good_reviews[L][1])\n",
    "        \n",
    "    #Make the train labels binary\n",
    "    train_labels = [1 if x >=4 else 0 for x in train_labels]\n",
    "    \n",
    "    #Make a FeatureUnion object with the desired features then fit to train reviews\n",
    "    test_results = {}\n",
    "    feature_selection = {\"sent_tf\":(True, True, False), \n",
    "                         \"sent\": (True,False,False),\n",
    "                         \"tf_lda\": (False,True,True), \n",
    "                         \"all\": (True, True, True)}\n",
    "\n",
    "    for feature in feature_selection.keys():\n",
    "        #Make a FeatureUnion object with the desired features then fit to train reviews\n",
    "        comb_features = yml.make_featureunion(sent_percent=feature_selection[feature][0], \n",
    "                                              tf = feature_selection[feature][1], \n",
    "                                              lda = feature_selection[feature][2])\n",
    "        if feature_selection[feature][1] == True:\n",
    "            #Create the Delta-TFIDF Feature\n",
    "            delta_vect = DeltaTfidfVectorizer(stop_words = 'english')\n",
    "            delta_tfidf_vect = delta_vect.fit_transform(sub_train_reviews,train_labels)\n",
    "            comb_features.fit(sub_train_reviews)\n",
    "            train_features = comb_features.transform(sub_train_reviews)\n",
    "            train_lsi = yml.get_lsi_features(sub_train_reviews, lsi, topics, dictionary)\n",
    "            train_features = sparse.hstack((train_features, train_lsi, delta_tfidf_vect))\n",
    "            train_features = train_features.todense() \n",
    "        else:\n",
    "            delta_vect = None\n",
    "            comb_features.fit(sub_train_reviews)\n",
    "            train_features = comb_features.transform(sub_train_reviews)\n",
    "            train_lsi = yml.get_lsi_features(sub_train_reviews, lsi, topics, dictionary)\n",
    "            train_features = sparse.hstack((train_features, train_lsi))\n",
    "            train_features = train_features.todense()\n",
    "\n",
    "        #Fit LSI model and return number of LSI topics\n",
    "        lsi, topics, dictionary = yml.fit_lsi(sub_train_reviews)\n",
    "        \n",
    "        #fit each model in turn \n",
    "        model_runs = {\"svm\": (True, False, False),\n",
    "                      \"rf\": (False, True, False), \n",
    "                      \"naive_bayes\": (False, False, True)}\n",
    "\n",
    "        for model_run in model_runs.keys():\n",
    "            clf = yml.fit_model(train_features, train_labels, svm_clf = model_runs[model_run][0], \n",
    "                            RandomForest = model_runs[model_run][1], \n",
    "                                nb = model_runs[model_run][2])\n",
    "            threshold = 0.7\n",
    "            error = yml.test_user_set(test_set, clf, restaurant_df, user_df, comb_features, \n",
    "                                      threshold, lsi, topics, dictionary, delta_vect)\n",
    "            test_results[(feature, model_run)] = (yml.get_log_loss(error), \n",
    "                                            yml.get_accuracy_score(error), \n",
    "                                            yml.get_precision_score(error))\n",
    "    \n",
    "    string_keys_dict = {}\n",
    "    for key in test_results.keys():\n",
    "        string_keys_dict[str(key)] = test_results[key]\n",
    "        \n",
    "    users_results[str(users.keys()[j])] = string_keys_dict\n",
    "    \n",
    "with open('test_results.json', 'wb') as fp:\n",
    "    json.dump(users_results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ELcQDlf69kb-ihJfxZyL0A': {\"('all', 'naive_bayes')\": (-0.5132591825971579,\n",
       "   0.38197424892703863,\n",
       "   0.47368421052631576),\n",
       "  \"('all', 'rf')\": (-0.13524381691250284, 0.38197424892703863, 0.0),\n",
       "  \"('all', 'svm')\": (-0.6231199308465194,\n",
       "   0.38626609442060084,\n",
       "   0.4807692307692308),\n",
       "  \"('sent', 'naive_bayes')\": (-0.310830361150146, 0.38626609442060084, 0.25),\n",
       "  \"('sent', 'rf')\": (-0.24608990999175764, 0.38197424892703863, 0.0),\n",
       "  \"('sent', 'svm')\": (-0.18768351923953727, 0.37339055793991416, 1.0),\n",
       "  \"('sent_tf', 'naive_bayes')\": (-0.5516115261545517,\n",
       "   0.4034334763948498,\n",
       "   0.375),\n",
       "  \"('sent_tf', 'rf')\": (-0.27731897823972373, 0.3776824034334764, 0.0),\n",
       "  \"('sent_tf', 'svm')\": (-0.6574780268983107,\n",
       "   0.3948497854077253,\n",
       "   0.4642857142857143),\n",
       "  \"('tf_lda', 'naive_bayes')\": (-0.546797861289129,\n",
       "   0.39914163090128757,\n",
       "   0.391304347826087),\n",
       "  \"('tf_lda', 'rf')\": (-0.2246986776670953, 0.3776824034334764, 0.0),\n",
       "  \"('tf_lda', 'svm')\": (-0.5046822580002839,\n",
       "   0.37339055793991416,\n",
       "   0.5238095238095238)}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_results"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
