{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from ast import literal_eval as make_tuple\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "sys.path.append('../machine_learning')\n",
    "import yelp_ml as yml\n",
    "reload(yml)\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Scrapped Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc_reviews = json.load(open(\"../Yelp_web_scrapper/dc_reviews.json\"))\n",
    "newyork_reviews = json.load(open(\"../Yelp_web_scrapper/newyork_reviews.json\"))\n",
    "austin_reviews = json.load(open(\"../Yelp_web_scrapper/austin_reviews.json\"))\n",
    "chicago_reviews = json.load(open(\"../Yelp_web_scrapper/chicago_reviews.json\"))\n",
    "la_reviews = json.load(open(\"../Yelp_web_scrapper/la_reviews.json\"))\n",
    "\n",
    "scrapped_reviews = {'dc': dc_reviews, 'ny': newyork_reviews, \n",
    "                    'austin': austin_reviews, 'chicago': chicago_reviews, \n",
    "                    'la': la_reviews}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Hu & Liu (2004) Word Dictionary and Wrangled Large Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lh_neg = open('../input/negative-words.txt', 'r').read()\n",
    "lh_neg = lh_neg.split('\\n')\n",
    "lh_pos = open('../input/positive-words.txt', 'r').read()\n",
    "lh_pos = lh_pos.split('\\n')\n",
    "users = json.load(open(\"cleaned_large_user_dictionary.json\"))\n",
    "word_list = list(set(lh_pos + lh_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the AWS Instance and get the restaurant reviews from the cleaned data database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip = '54.175.170.119'\n",
    "conn = MongoClient(ip, 27017)\n",
    "conn.database_names()\n",
    "db = conn.get_database('cleaned_data')\n",
    "reviews = db.get_collection('restaurant_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "1. Supply User ID\n",
    "2. Get all restaurant IDs that the user has reviewed\n",
    "3. Do a random 75 (training)/25(testing) split of the restaurant IDs\n",
    "4. For the training sample, get all of the **user's** reviews for those restaurants\n",
    "5. For each restaurant in the testing sample, get all of that **restaurant's** reviews\n",
    "6. Train each of the (feature, model) combinations on the **reviews** in the training sample\n",
    "7. For each **review** in the testing sample, classify that review using the model\n",
    "    1. If the total proportion of positive reviews is greater than 70% for **each** restaurant, classify that **restaurant's** rating as positive.\n",
    "    2. Else, classify  that **restaurant's** rating as negative\n",
    "8. For each restaurant's predicted rating, check against what the **user** _actually_ thought 9. Use these to determine log-loss, accuracy, and precision \n",
    "\n",
    "The features and models we use are:\n",
    "\n",
    "#### Features:\n",
    "                                 \n",
    "1. (Sentiment %, TF-IDF w/ (2,2) N-Gram, LSA)\n",
    "2. (Sentiment %, LSA)\n",
    "3. (TF-IDF w/ (2,2) N-Gram, LDA, LSA)\n",
    "4. (Sentiment %, TF-IDF w/ (2,2) N-Gram, LDA, LSA)\n",
    "                  \n",
    "#### Models:\n",
    "1. Linear Support Vector Machine\n",
    "2. Random Forest\n",
    "3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "string_keys_dict = {}\n",
    "for j in tqdm.tqdm(range(146, 147)):\n",
    "    \n",
    "    #Generate a dataframe that has the user's review text, review rating, and restaurant ID\n",
    "    test_results = {}\n",
    "    user_df = yml.make_user_df(users[users.keys()[j]])\n",
    "    \n",
    "    #Only predict for the user if they have at least 20 bad ratings\n",
    "    if len([x for x in user_df['rating'] if x < 4]) < 20:\n",
    "        string_keys_dict[str(users.keys()[j])] = test_results\n",
    "        continue\n",
    "    else:\n",
    "        business_ids = list(set(user_df['biz_id']))\n",
    "        restreview = {}\n",
    "        \n",
    "        #Create a training and test sample from the user reviewed restaurants\n",
    "        #using a random 25% subset of all the restaurants the user has reviewed\n",
    "        split_samp = .25\n",
    "        len_random = int(len(business_ids) * split_samp)\n",
    "        test_set = random.sample(business_ids, len_random)\n",
    "        training_set = [x for x in business_ids if x not in test_set]\n",
    "        sub_train_reviews, train_labels, train_reviews, train_ratings = [], [], [], []\n",
    "\n",
    "        #Create a list with the tuple (training review, training rating) \n",
    "        for rest_id in training_set:\n",
    "            train_reviews.append((user_df[user_df['biz_id'] == rest_id]['review_text'].iloc[0],\n",
    "                                     user_df[user_df['biz_id'] == rest_id]['rating'].iloc[0]))\n",
    "\n",
    "        #Note that the distribution is heavily skewed towards good reviews. \n",
    "        #Therefore, we create a training sample with the same amount of\n",
    "        #positive and negative reviews\n",
    "        sample_size = min(len([x[1] for x in train_reviews if x[1] < 4]),\n",
    "                              len([x[1] for x in train_reviews if x[1] >= 4]))\n",
    "\n",
    "        bad_reviews = [x for x in train_reviews if x[1] < 4]\n",
    "        good_reviews = [x for x in train_reviews if x[1] >= 4]\n",
    "\n",
    "        for L in range(0, int(float(sample_size)/float(2))):\n",
    "            sub_train_reviews.append(bad_reviews[L][0])\n",
    "            sub_train_reviews.append(good_reviews[L][0])\n",
    "            train_labels.append(bad_reviews[L][1])\n",
    "            train_labels.append(good_reviews[L][1])\n",
    "\n",
    "        #Make the train labels binary\n",
    "        train_labels = [1 if x >=4 else 0 for x in train_labels]\n",
    "        \n",
    "        #Sanity check for non-empty training reviews\n",
    "        if not sub_train_reviews:\n",
    "            string_keys_dict[str(users.keys()[j])] = test_results\n",
    "            continue\n",
    "        else:\n",
    "            for i in range(0, len(business_ids)):\n",
    "                rlist = []\n",
    "                for obj in reviews.find({'business_id':business_ids[i]}):\n",
    "                    rlist.append(obj)\n",
    "                restreview[business_ids[i]] = rlist\n",
    "\n",
    "            restaurant_df = yml.make_biz_df(users.keys()[j], restreview)\n",
    "\n",
    "            #Make a FeatureUnion object with the desired features then fit to train reviews\n",
    "            feature_selection = {\"sent_tf\":(True, True, False), \n",
    "                                 \"sent\": (True,False,False),\n",
    "                                 \"tf_lda\": (False,True,True), \n",
    "                                 \"all\": (True, True, True)}\n",
    "\n",
    "            for feature in feature_selection.keys():\n",
    "                #Make a FeatureUnion object with the desired features then fit to train reviews\n",
    "                comb_features = yml.make_featureunion(sent_percent=feature_selection[feature][0], \n",
    "                                                      tf = feature_selection[feature][1], \n",
    "                                                      lda = feature_selection[feature][2])\n",
    "\n",
    "                delta_vect = None\n",
    "                comb_features.fit(sub_train_reviews)\n",
    "                train_features = comb_features.transform(sub_train_reviews)\n",
    "\n",
    "                #Fit LSI model and return number of LSI topics\n",
    "                lsi, topics, dictionary = yml.fit_lsi(sub_train_reviews)\n",
    "                train_lsi = yml.get_lsi_features(sub_train_reviews, lsi, topics, dictionary)\n",
    "\n",
    "                #Stack the LSI and combined features together\n",
    "                train_features = sparse.hstack((train_features, train_lsi))\n",
    "                train_features = train_features.todense()\n",
    "\n",
    "                #fit each model in turn \n",
    "                model_runs = {\"svm\": (True, False, False),\n",
    "                              \"rf\": (False, True, False), \n",
    "                              \"naive_bayes\": (False, False, True)}\n",
    "\n",
    "                for model_run in model_runs.keys():\n",
    "                    clf = yml.fit_model(train_features, train_labels, svm_clf = model_runs[model_run][0], \n",
    "                                    RandomForest = model_runs[model_run][1], \n",
    "                                        nb = model_runs[model_run][2])\n",
    "                    threshold = 0.7\n",
    "                    error = yml.test_user_set(test_set, clf, restaurant_df, user_df, comb_features, \n",
    "                                              threshold, lsi, topics, dictionary, delta_vect)\n",
    "                    test_results[str((feature, model_run))] = (yml.get_log_loss(error), \n",
    "                                                    yml.get_accuracy_score(error), \n",
    "                                                    yml.get_precision_score(error))\n",
    "                \n",
    "    string_keys_dict[str(users.keys()[j])] = test_results\n",
    "            \n",
    "with open('test_results.json', 'wb') as fp:\n",
    "    json.dump(string_keys_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best performing (feature, model) combination for this user was a combination of **all** of the features in a linear support vector machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame.from_dict(test_results, orient = 'index')\n",
    "test_results_df['model'] = test_results_df.index\n",
    "test_results_df.columns = ['log_loss', 'accuracy', 'precision', 'model']\n",
    "test_results_df.plot(x = 'model', y = ['log_loss', 'accuracy', 'precision'], kind = 'bar')\n",
    "ax = plt.subplot(111)\n",
    "ax.legend(bbox_to_anchor=(1.4, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_results = []\n",
    "#Get feature and model combination that yields the highest precision\n",
    "for key in test_results.keys():\n",
    "    feat_model = make_tuple(key)\n",
    "    if not top_results:\n",
    "        top_results = [(feat_model,test_results[key][2])]\n",
    "    else:\n",
    "        if test_results[key][2] > top_results[0][1]:\n",
    "            top_results.pop()\n",
    "            top_results = [(feat_model, test_results[key][2])]\n",
    "# feat_result = top_results[0][0][0]\n",
    "# model_result = top_results[0][0][1]\n",
    "feat_result = 'tf_lda'\n",
    "model_result = 'svm'\n",
    "for j in tqdm.tqdm(range(146, 147)):\n",
    "    user_df = yml.make_user_df(users[users.keys()[j]])\n",
    "    business_ids = list(set(user_df['biz_id']))\n",
    "\n",
    "    #Create a list of training reviews and training ratings\n",
    "    for rest_id in business_ids:\n",
    "        train_reviews.append((user_df[user_df['biz_id'] == rest_id]['review_text'].iloc[0],\n",
    "                                 user_df[user_df['biz_id'] == rest_id]['rating'].iloc[0]))\n",
    "\n",
    "    #Create an even sample s.t. len(positive_reviews) = len(negative_reviews)\n",
    "    sample_size = min(len([x[1] for x in train_reviews if x[1] < 4]),\n",
    "                          len([x[1] for x in train_reviews if x[1] >= 4]))\n",
    "    \n",
    "    bad_reviews = [x for x in train_reviews if x[1] < 4]\n",
    "    good_reviews = [x for x in train_reviews if x[1] >= 4]\n",
    "    \n",
    "    train_labels = []\n",
    "    sub_train_reviews = []\n",
    "    for L in range(0, int(float(sample_size)/float(2))):\n",
    "        sub_train_reviews.append(bad_reviews[L][0])\n",
    "        sub_train_reviews.append(good_reviews[L][0])\n",
    "        train_labels.append(bad_reviews[L][1])\n",
    "        train_labels.append(good_reviews[L][1])\n",
    "        \n",
    "    #Make the train labels binary\n",
    "    train_labels = [1 if x >=4 else 0 for x in train_labels]\n",
    "    \n",
    "    #Fit LSI model and return number of LSI topics\n",
    "    lsi, topics, dictionary = yml.fit_lsi(sub_train_reviews)\n",
    "\n",
    "    #Make a FeatureUnion object with the desired features then fit to train reviews\n",
    "    feature_selection = {\"sent_tf\":(True, True, False), \n",
    "                         \"sent\": (True,False,False),\n",
    "                         \"tf_lda\": (False,True,True), \n",
    "                         \"all\": (True, True, True)}\n",
    "    top_feature = feature_selection['all']\n",
    "    \n",
    "    comb_features = yml.make_featureunion(sent_percent=top_feature[0], \n",
    "                                          tf = top_feature[1], \n",
    "                                          lda = top_feature[2])\n",
    "        \n",
    "    comb_features.fit(sub_train_reviews)\n",
    "    train_features = comb_features.transform(sub_train_reviews)\n",
    "    train_lsi = yml.get_lsi_features(sub_train_reviews, lsi, topics, dictionary)\n",
    "    train_features = sparse.hstack((train_features, train_lsi))\n",
    "    train_features = train_features.todense()\n",
    "\n",
    "    #Fit LSI model and return number of LSI topics\n",
    "    lsi, topics, dictionary = yml.fit_lsi(sub_train_reviews)\n",
    "        \n",
    "    #Get the top performing model and fit using that model\n",
    "    model_runs = {\"svm\": (True, False, False),\n",
    "                  \"rf\": (False, True, False), \n",
    "                  \"naive_bayes\": (False, False, True)}\n",
    "    \n",
    "    top_model = model_runs['svm']\n",
    "    clf = yml.fit_model(train_features, train_labels, svm_clf = top_model[0], \n",
    "                RandomForest = top_model[1], \n",
    "                    nb = top_model[2])\n",
    "\n",
    "    threshold = 0.7\n",
    "    user_results = {}\n",
    "    for key in scrapped_reviews.keys():\n",
    "        user_results[key] = yml.make_rec(scrapped_reviews[key], clf, threshold, comb_features, \n",
    "                                lsi, topics, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "#Collect the results into a list of tuples, then select the top\n",
    "#5 most confident good recs and top 5 most confident bad recs \n",
    "#for each location\n",
    "################################################################\n",
    "tuple_results = {}\n",
    "for key in user_results.keys():\n",
    "    tuple_results[key] = []\n",
    "    for result in user_results[key]:\n",
    "        tuple_results[key].append((result[1], result[2], result[3]))\n",
    "    tuple_results[key] = sorted(tuple_results[key], key=lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the top 5 restaurants for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in tuple_results.keys():\n",
    "    print \"The top 5 recommendations for \" + key + \" are: \"\n",
    "    print tuple_results[key][-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the bottom 5 restaurants for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in tuple_results.keys():\n",
    "    print \"The bottom 5 recommendations for \" + key + \" are: \"\n",
    "    print tuple_results[key][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a step back and look at the user's word choice and tone. First let's look at the user_df dataframe, which contains all of the user's reviews and ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_df = yml.make_user_df(users[users.keys()[j]])\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the most important TF-IDF features for our linear SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "tfv = TfidfVectorizer(ngram_range = (2,2), stop_words = 'english')\n",
    "tfv.fit(sub_train_reviews)\n",
    "\n",
    "X_train = tfv.transform(sub_train_reviews)\n",
    "ex_clf = svm.LinearSVC()\n",
    "ex_clf.fit(X_train, train_labels)\n",
    "yml.plot_coefficients(ex_clf, tfv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the topics generated by LDA. Specifically, we focus only on the good reviews and plot the topics that appeared most often in the good reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's take a look at sample weightings for the user's GOOD reviews\n",
    "good_reviews = [a for (a,b) in zip(sub_train_reviews, train_labels) if b == 1]\n",
    "tf = vectorizer.fit(sub_train_reviews)\n",
    "lda_fit = LatentDirichletAllocation(n_topics=50).fit(tf.transform(sub_train_reviews))\n",
    "\n",
    "tf_good = tf.transform(good_reviews)\n",
    "lda_good = lda_fit.transform(tf_good)\n",
    "\n",
    "#Take the average of each topic weighting amongst good reviews and graph each topic\n",
    "topic_strings = [\"Topic \" + str(x) for x in range(0,50)]\n",
    "\n",
    "topic_dict = {}\n",
    "for review in lda_good:\n",
    "    for x in range(0,50):\n",
    "        try:\n",
    "            topic_dict[topic_strings[x]].append(review[x])\n",
    "        except:\n",
    "            topic_dict[topic_strings[x]] = [review[x]]\n",
    "            \n",
    "average_top_weight = {}\n",
    "for x in range(0,50):\n",
    "    average_top_weight[topic_strings[x]] = reduce(lambda x, y: x + y, topic_dict[topic_strings[x]]) \n",
    "    / len(topic_dict[topic_strings[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############    \n",
    "#Plot the average weights for each topic in the good reviews\n",
    "##############     \n",
    "#Find the average topic weights for each topic\n",
    "average_topics = pd.DataFrame.from_dict(average_top_weight, orient = 'index')\n",
    "average_topics.columns = ['topic_weight']\n",
    "average_topics['topic'] = average_topics.index\n",
    "average_topics['topic'] = [int(x[5:8]) for x in average_topics['topic']]\n",
    "average_topics = average_topics.sort_values(['topic'])\n",
    "x_max = average_topics.sort_values('topic_weight')['topic_weight'][-2] + 1\n",
    "\n",
    "#Make the plot\n",
    "good_plt = average_topics.plot(x='topic', y='topic_weight', kind='scatter', legend=False)\n",
    "yml.label_point(average_topics.topic, average_topics.topic_weight, good_plt)\n",
    "good_plt.set_ylim(0, x_max)\n",
    "good_plt.set_xlim(0, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the top 5 words in each of these topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#View the top words in the outlier topics from the LDA representation\n",
    "yml.label_point(average_topics.topic, average_topics.topic_weight, good_plt)\n",
    "\n",
    "a = pd.concat({'x': average_topics.topic, \n",
    "                   'y': average_topics.topic_weight}, axis=1)\n",
    "top_topics = [a[a['y'] == max(a['y'])]['x'][0]]\n",
    "a = a[a['y'] != max(a['y'])]\n",
    "for i, point in a.iterrows():\n",
    "    if (point['y'] > (a['y'].mean() + 1.5 * a['y'].std()) ):\n",
    "        top_topics.append(int(point['x']))\n",
    "            \n",
    "\n",
    "#Display top words in each topic                     \n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "yml.display_topics(lda_fit, tf_feature_names, no_top_words, top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's take a look at what kind of restaurants the user likes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_restaurants = user_df[user_df['rating'] >= 4]['biz_id']\n",
    "bis_data = db.get_collection('restaurants')\n",
    "#Pull each restaurant attribute from the MongoDB\n",
    "restreview_good = {}\n",
    "for i in tqdm.tqdm(range(0, len(good_restaurants))):\n",
    "    rlist = []\n",
    "    for obj in bis_data.find({'business_id':business_ids[i]}):\n",
    "        rlist.append(obj)\n",
    "    restreview_good[business_ids[i]] = rlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get all the categories for the good restaurants \n",
    "good_list = []\n",
    "for key in restreview_good.keys():\n",
    "    good_list.extend(restreview_good[key][0]['categories'])\n",
    "good_list = [word for word in good_list if (word != u'Restaurants')]\n",
    "good_list = [word for word in good_list if (word != u'Food')]\n",
    "unique_categories = list(set(good_list))\n",
    "category_count = [good_list.count(cat) for cat in unique_categories]\n",
    "category_list = [(a,b) for (a,b) in zip(unique_categories, category_count) if b >= 10]\n",
    "unique_categories = [a for (a,b) in category_list] \n",
    "category_count = [b for (a,b) in category_list]\n",
    "biz_category = pd.DataFrame({'category': unique_categories, 'count': category_count})\n",
    "\n",
    "#Plot only categories that show up at least 10 times\n",
    "good_plt = biz_category.plot(x='category', y='count', kind='bar', legend=False)\n",
    "good_plt.set_ylim(0,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The restaurants that we've recommended include many traditional and new American restaurants, as well as Burger places. This suggests that our recommendation system does a good job picking up on the latent preferences of the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LSA, we arbitrarily chose the number of topics to be the mean of the singular values. This is a meaningless choice and completely arbitrary.\n",
    "\n",
    "The words in the below representation represent word vectors in the initial term-document matrix, the coefficients on each word vector is a result of the singular value decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"The number of LSA topics is: \" + str(lsi.num_topics)\n",
    "for x in lsi.show_topics():\n",
    "    print str(x) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's take a look at sample weightings for the user's GOOD reviews\n",
    "good_reviews = [a for (a,b) in zip(sub_train_reviews, train_labels) if b == 1]\n",
    "lsi, topics, dictionary = yml.fit_lsi(good_reviews)\n",
    "train_lsi = yml.get_lsi_features(good_reviews, lsi, topics, dictionary).todense()\n",
    "train_lsi = [np.array(x[0])[0] for x in train_lsi]\n",
    "\n",
    "#Take the average of each topic weighting amongst good reviews and graph each topic\n",
    "lsi_topic_strings = [\"Topic \" + str(x) for x in range(0,9)]\n",
    "\n",
    "lsi_topic_dict = {}\n",
    "for review in train_lsi:\n",
    "    for x in range(0,9):\n",
    "        try:\n",
    "            lsi_topic_dict[lsi_topic_strings[x]].append(review[x])\n",
    "        except:\n",
    "            lsi_topic_dict[lsi_topic_strings[x]] = [review[x]]\n",
    "            \n",
    "average_lsi_weight = {}\n",
    "for x in range(0,9):\n",
    "    average_lsi_weight[lsi_topic_strings[x]] = reduce(lambda x, y: x + y, \n",
    "                                                      lsi_topic_dict[lsi_topic_strings[x]]) \n",
    "    / len(lsi_topic_dict[lsi_topic_strings[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############    \n",
    "#Plot the average weights for each topic in the good reviews\n",
    "##############     \n",
    "#Find the average topic weights for each topic\n",
    "lsi_average_topics = pd.DataFrame.from_dict(average_lsi_weight, orient = 'index')\n",
    "lsi_average_topics.columns = ['topic_weight']\n",
    "lsi_average_topics['topic'] = lsi_average_topics.index\n",
    "lsi_average_topics['topic'] = [int(x[5:8]) for x in lsi_average_topics['topic']]\n",
    "lsi_average_topics = lsi_average_topics.sort_values(['topic'])\n",
    "x_max = lsi_average_topics.sort_values('topic_weight')['topic_weight'][-2] + 1\n",
    "\n",
    "#Make the plot\n",
    "lsi_good_plt = lsi_average_topics.plot(x='topic', y='topic_weight', kind='scatter', legend=False)\n",
    "yml.label_point(lsi_average_topics.topic, lsi_average_topics.topic_weight, lsi_good_plt)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
