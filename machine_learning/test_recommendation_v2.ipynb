{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from ast import literal_eval as make_tuple\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_deltatfidf import DeltaTfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "sys.path.append('../machine_learning')\n",
    "import yelp_ml as yml\n",
    "reload(yml)\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import scrapped reviews\n",
    "dc_reviews = json.load(open(\"../Yelp_web_scrapper/dc_reviews.json\"))\n",
    "newyork_reviews = json.load(open(\"../Yelp_web_scrapper/newyork_reviews.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import word dictionaries\n",
    "lh_neg = open('../input/negative-words.txt', 'r').read()\n",
    "lh_neg = lh_neg.split('\\n')\n",
    "lh_pos = open('../input/positive-words.txt', 'r').read()\n",
    "lh_pos = lh_pos.split('\\n')\n",
    "users = json.load(open(\"cleaned_large_user_dictionary.json\"))\n",
    "word_list = list(set(lh_pos + lh_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-72-fed646b0b0ef>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-72-fed646b0b0ef>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    for i in range(0, len(business_ids)):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#####Pull restaurant data for a given user\n",
    "ip = '54.175.170.119'\n",
    "conn = MongoClient(ip, 27017)\n",
    "conn.database_names()\n",
    "db = conn.get_database('cleaned_data')\n",
    "reviews = db.get_collection('restaurant_reviews')\n",
    "string_keys_dict = {}\n",
    "for j in tqdm.tqdm(range(16, 17)):\n",
    "    test_results = {}\n",
    "    user_df = yml.make_user_df(users[users.keys()[j]])\n",
    "    \n",
    "    if len([x for x in user_df['rating'] if x < 4]) < 20:\n",
    "        string_keys_dict[str(users.keys()[j])] = test_results\n",
    "        continue\n",
    "    else:\n",
    "        business_ids = list(set(user_df['biz_id']))\n",
    "        restreview = {}\n",
    "        \n",
    "        #Create a training and test sample from the user reviewed restaurants\n",
    "        split_samp = .25\n",
    "        len_random = int(len(business_ids) * split_samp)\n",
    "        test_set = random.sample(business_ids, len_random)\n",
    "        training_set = [x for x in business_ids if x not in test_set]\n",
    "        sub_train_reviews, train_labels, train_reviews, train_ratings = [], [], [], []\n",
    "\n",
    "        #Create a list of training reviews and training ratings\n",
    "        for rest_id in training_set:\n",
    "            train_reviews.append((user_df[user_df['biz_id'] == rest_id]['review_text'].iloc[0],\n",
    "                                     user_df[user_df['biz_id'] == rest_id]['rating'].iloc[0]))\n",
    "\n",
    "        #Create an even sample s.t. len(positive_reviews) = len(negative_reviews)\n",
    "        sample_size = min(len([x[1] for x in train_reviews if x[1] < 4]),\n",
    "                              len([x[1] for x in train_reviews if x[1] >= 4]))\n",
    "\n",
    "        bad_reviews = [x for x in train_reviews if x[1] < 4]\n",
    "        good_reviews = [x for x in train_reviews if x[1] >= 4]\n",
    "\n",
    "        for L in range(0, int(float(sample_size)/float(2))):\n",
    "            sub_train_reviews.append(bad_reviews[L][0])\n",
    "            sub_train_reviews.append(good_reviews[L][0])\n",
    "            train_labels.append(bad_reviews[L][1])\n",
    "            train_labels.append(good_reviews[L][1])\n",
    "\n",
    "        #Make the train labels binary\n",
    "        train_labels = [1 if x >=4 else 0 for x in train_labels]\n",
    "        \n",
    "        if not sub_train_reviews:\n",
    "            string_keys_dict[str(users.keys()[j])] = test_results\n",
    "            continue\n",
    "        else:\n",
    "        for i in range(0, len(business_ids)):\n",
    "            rlist = []\n",
    "            for obj in reviews.find({'business_id':business_ids[i]}):\n",
    "                rlist.append(obj)\n",
    "            restreview[business_ids[i]] = rlist\n",
    "\n",
    "        restaurant_df = yml.make_biz_df(users.keys()[j], restreview)\n",
    "\n",
    "        #Make a FeatureUnion object with the desired features then fit to train reviews\n",
    "        feature_selection = {\"sent_tf\":(True, True, False), \n",
    "                             \"sent\": (True,False,False),\n",
    "                             \"tf_lda\": (False,True,True), \n",
    "                             \"all\": (True, True, True)}\n",
    "\n",
    "        for feature in feature_selection.keys():\n",
    "            #Make a FeatureUnion object with the desired features then fit to train reviews\n",
    "            comb_features = yml.make_featureunion(sent_percent=feature_selection[feature][0], \n",
    "                                                  tf = feature_selection[feature][1], \n",
    "                                                  lda = feature_selection[feature][2])\n",
    "\n",
    "            delta_vect = None\n",
    "            comb_features.fit(sub_train_reviews)\n",
    "            train_features = comb_features.transform(sub_train_reviews)\n",
    "\n",
    "            #Fit LSI model and return number of LSI topics\n",
    "            lsi, topics, dictionary = yml.fit_lsi(sub_train_reviews)\n",
    "            train_lsi = yml.get_lsi_features(sub_train_reviews, lsi, topics, dictionary)\n",
    "\n",
    "            #Stack the LSI and combined features together\n",
    "            train_features = sparse.hstack((train_features, train_lsi))\n",
    "            train_features = train_features.todense()\n",
    "\n",
    "            #fit each model in turn \n",
    "            model_runs = {\"svm\": (True, False, False),\n",
    "                          \"rf\": (False, True, False), \n",
    "                          \"naive_bayes\": (False, False, True)}\n",
    "\n",
    "            for model_run in model_runs.keys():\n",
    "                clf = yml.fit_model(train_features, train_labels, svm_clf = model_runs[model_run][0], \n",
    "                                RandomForest = model_runs[model_run][1], \n",
    "                                    nb = model_runs[model_run][2])\n",
    "                threshold = 0.7\n",
    "                error = yml.test_user_set(test_set, clf, restaurant_df, user_df, comb_features, \n",
    "                                          threshold, lsi, topics, dictionary, delta_vect)\n",
    "                test_results[str((feature, model_run))] = (yml.get_log_loss(error), \n",
    "                                                yml.get_accuracy_score(error), \n",
    "                                                yml.get_precision_score(error))\n",
    "                \n",
    "    string_keys_dict[str(users.keys()[j])] = test_results\n",
    "            \n",
    "with open('test_results.json', 'wb') as fp:\n",
    "    json.dump(string_keys_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/robertsonwang/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/Users/robertsonwang/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:764: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  feature_idx = vocabulary[feature]\n",
      "100%|██████████| 1/1 [00:34<00:00, 34.10s/it]\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#Make a Recommendation\n",
    "#########################\n",
    "top_results = []\n",
    "#Get feature and model combination that yields the highest precision\n",
    "for key in test_results.keys():\n",
    "    feat_model = make_tuple(key)\n",
    "    if not top_results:\n",
    "        top_results = [(feat_model,test_results[key][2])]\n",
    "    else:\n",
    "        if test_results[key][2] > top_results[0][1]:\n",
    "            top_results.pop()\n",
    "            top_results = [(feat_model, test_results[key][2])]\n",
    "feat_result = top_results[0][0][0]\n",
    "model_result = top_results[0][0][1]\n",
    "\n",
    "for j in tqdm.tqdm(range(large_user_number, large_user_number+1)):\n",
    "    user_df = yml.make_user_df(users[users.keys()[j]])\n",
    "    business_ids = list(set(user_df['biz_id']))\n",
    "\n",
    "    #Create a list of training reviews and training ratings\n",
    "    for rest_id in business_ids:\n",
    "        train_reviews.append((user_df[user_df['biz_id'] == rest_id]['review_text'].iloc[0],\n",
    "                                 user_df[user_df['biz_id'] == rest_id]['rating'].iloc[0]))\n",
    "\n",
    "    #Create an even sample s.t. len(positive_reviews) = len(negative_reviews)\n",
    "    sample_size = min(len([x[1] for x in train_reviews if x[1] < 4]),\n",
    "                          len([x[1] for x in train_reviews if x[1] >= 4]))\n",
    "    \n",
    "    bad_reviews = [x for x in train_reviews if x[1] < 4]\n",
    "    good_reviews = [x for x in train_reviews if x[1] >= 4]\n",
    "    \n",
    "    train_labels = []\n",
    "    sub_train_reviews = []\n",
    "    for L in range(0, int(float(sample_size)/float(2))):\n",
    "        sub_train_reviews.append(bad_reviews[L][0])\n",
    "        sub_train_reviews.append(good_reviews[L][0])\n",
    "        train_labels.append(bad_reviews[L][1])\n",
    "        train_labels.append(good_reviews[L][1])\n",
    "        \n",
    "    #Make the train labels binary\n",
    "    train_labels = [1 if x >=4 else 0 for x in train_labels]\n",
    "    \n",
    "    #Fit LSI model and return number of LSI topics\n",
    "    lsi, topics, dictionary = yml.fit_lsi(sub_train_reviews)\n",
    "\n",
    "    #Make a FeatureUnion object with the desired features then fit to train reviews\n",
    "    feature_selection = {\"sent_tf\":(True, True, False), \n",
    "                         \"sent\": (True,False,False),\n",
    "                         \"tf_lda\": (False,True,True), \n",
    "                         \"all\": (True, True, True)}\n",
    "    top_feature = feature_selection['all']\n",
    "    \n",
    "    comb_features = yml.make_featureunion(sent_percent=top_feature[0], \n",
    "                                          tf = top_feature[1], \n",
    "                                          lda = top_feature[2])\n",
    "        \n",
    "    comb_features.fit(sub_train_reviews)\n",
    "    train_features = comb_features.transform(sub_train_reviews)\n",
    "    train_lsi = yml.get_lsi_features(sub_train_reviews, lsi, topics, dictionary)\n",
    "    train_features = sparse.hstack((train_features, train_lsi))\n",
    "    train_features = train_features.todense()\n",
    "\n",
    "    #Fit LSI model and return number of LSI topics\n",
    "    lsi, topics, dictionary = yml.fit_lsi(sub_train_reviews)\n",
    "        \n",
    "    #Get the top performing model and fit using that model\n",
    "    model_runs = {\"svm\": (True, False, False),\n",
    "                  \"rf\": (False, True, False), \n",
    "                  \"naive_bayes\": (False, False, True)}\n",
    "    top_model = model_runs['svm']\n",
    "    clf = yml.fit_model(train_features, train_labels, svm_clf = top_model[0], \n",
    "                RandomForest = top_model[1], \n",
    "                    nb = top_model[2])\n",
    "\n",
    "    threshold = 0.7\n",
    "    user_results = yml.make_rec(dc_reviews, clf, threshold, comb_features, \n",
    "                                lsi, topics, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "#Collect the results into a list of tuples, then select the top\n",
    "#5 most confident good recs and top 5 most confident bad recs\n",
    "################################################################\n",
    "\n",
    "tuple_results = []\n",
    "for result in user_results:\n",
    "    tuple_results.append((result[1], result[2], result[3]))\n",
    "    \n",
    "#Sort the list of tuples by predicition confidence\n",
    "tuple_results = sorted(tuple_results, key=lambda tup: tup[1])\n",
    "top_5 = tuple_results[-5:]\n",
    "bottom_5 = tuple_results[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'/biz/kingbird-washington', 1.0, 1),\n",
       " (u'/biz/jojo-restaurant-and-bar-washington', 1.0, 1),\n",
       " (u'/biz/al-volo-washington', 1.0, 1),\n",
       " (u'/biz/timber-pizza-company-washington', 1.0, 1),\n",
       " (u'/biz/little-sesame-washington', 1.0, 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the top 5 restaurants\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'/biz/pier-2934-cajun-seafood-washington', 0.5, 0),\n",
       " (u'/biz/momofuku-ccdc-washington', 0.55000000000000004, 0),\n",
       " (u'/biz/laliguras-indian-and-nepali-bistro-washington-2',\n",
       "  0.55000000000000004,\n",
       "  0),\n",
       " (u'/biz/ben-tre-washington', 0.55000000000000004, 0),\n",
       " (u'/biz/dumplings-and-beyond-washington', 0.55000000000000004, 0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the bottom 5 restaurants\n",
    "bottom_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biz_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7ReFxabYRuDBmx9Bdx7VMA</td>\n",
       "      <td>4</td>\n",
       "      <td>Ok I know it was a fast food but they have exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>430DW6yItFj3iB710i1a8A</td>\n",
       "      <td>5</td>\n",
       "      <td>YUM YUM YUM YUM  Birthday cake is my favorite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7cIsnVpbiIpVkXZCMJyAtg</td>\n",
       "      <td>4</td>\n",
       "      <td>Fancy inside  lounge area with a tv kids area ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L9boPSsWE93vkRHR4ti2Fw</td>\n",
       "      <td>4</td>\n",
       "      <td>My brother suggested for us to try to this pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HH26SnOm2Ab7UfkNhQU66A</td>\n",
       "      <td>2</td>\n",
       "      <td>I swear the pizza was better before     It was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   biz_id  rating  \\\n",
       "0  7ReFxabYRuDBmx9Bdx7VMA       4   \n",
       "1  430DW6yItFj3iB710i1a8A       5   \n",
       "2  7cIsnVpbiIpVkXZCMJyAtg       4   \n",
       "3  L9boPSsWE93vkRHR4ti2Fw       4   \n",
       "4  HH26SnOm2Ab7UfkNhQU66A       2   \n",
       "\n",
       "                                         review_text  \n",
       "0  Ok I know it was a fast food but they have exc...  \n",
       "1  YUM YUM YUM YUM  Birthday cake is my favorite ...  \n",
       "2  Fancy inside  lounge area with a tv kids area ...  \n",
       "3  My brother suggested for us to try to this pla...  \n",
       "4  I swear the pizza was better before     It was...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#user_df contains all of the user's reviews, with one column of restaurants,\n",
    "#one column of the user's ratings, and one column with the user's reviews\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-9889b693ec70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mno_top_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf_feature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_feature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_top_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "#View the top words in the LDA representation\n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "display_topics(lda_fit, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapsed Gibbs Sampling\n",
    "\n",
    "1. Go through each document, and randomly assign each word in the document to one of the K topics.\n",
    "2. For each document d, go through each word w in d, and for each topic t:\n",
    "    1. Compute P(topic t | document d) = the proportion of words in document d that are currently assigned to topic t, and 2) p(word w | topic t) = the proportion of assignments to topic t over all documents that come from this word w. Reassign w a new topic, where you choose topic t with probability p(topic t | document d) * p(word w | topic t) (according to our generative model, this is essentially the probability that topic t generated word w, so it makes sense that we resample the current word's topic with this probability). (Also, I'm glossing over a couple of things here, such as the use of priors/pseudocounts in these probabilities.)\n",
    "........In other words, in this step, we're assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.\n",
    "After repeating the previous step a large number of times, you'll eventually reach a roughly steady state where your assignments are pretty good. So use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
